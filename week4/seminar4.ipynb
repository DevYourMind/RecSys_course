{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data from https://disk.yandex.ru/d/0ya1tUYrin_tEg \n",
    "# or https://www.kaggle.com/chiranjivdas09/ta-feng-grocery-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ta_feng_all_months_merged.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['CUSTOMER_ID', 'PRODUCT_ID', 'TRANSACTION_DT']]\n",
    "df = df.rename({'CUSTOMER_ID': 'user_id', 'PRODUCT_ID': 'item_id', 'TRANSACTION_DT': 'timestamp'}, axis=1)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.sort_values(['user_id', 'timestamp'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Удалим непопулярные айтемы и юзеров у которых мало транзакций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Оставим товары которые купили больше 100 раз\n",
    "\n",
    "* Оставим юзеров у которых больше 20 покупок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df) == 296132, 'wrong length of dataframe'\n",
    "assert df['user_id'].nunique() == 7070, 'incorrent number of users'\n",
    "assert df['item_id'].nunique() == 1774, 'incorrent number of items'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* закодируем товары числами от 1 до n\n",
    "* закодируем юзеров числами от 1 до m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df['item_id'].min() == 1, 'item encoding should start from 1'\n",
    "assert df['user_id'].min() == 1, 'user encoding should start from 1'\n",
    "assert df['item_id'].max() == 1774\n",
    "assert df['user_id'].max() == 7070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* сгруппируем корзины пользователя(все айтемы купленный в один день)\n",
    "* оставим только тех пользователей, у которых есть хотя бы 5 корзин\n",
    "* выделим последнюю корзину как таргет\n",
    "\n",
    "пример:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| user_id | item_id | timestamp  |\n",
    "|---------|---------|------------|\n",
    "| 1       | 1       | 2000-12-03 |\n",
    "| 1       | 2       | 2000-12-03 |\n",
    "| 1       | 3       | 2000-12-04 |\n",
    "| 1       | 5       | 2000-12-04 |\n",
    "| 1       | 7       | 2000-12-04 |\n",
    "| 1       | 4       | 2000-12-06 |\n",
    "| 1       | 3       | 2000-12-09 |\n",
    "| 1       | 4       | 2000-12-14 |\n",
    "| 1       | 5       | 2000-12-14 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| user_id \t| baskets \t| target  \t|\n",
    "|---------\t|---------\t|------------\t|\n",
    "| 1       \t| [[1, 2], [3, 5, 7], [4], [3]]       \t| [4, 5]\t|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_baskets(df):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "df_grouped = group_baskets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_grouped['baskets'].apply(len).sum() == 45075, 'wrong number of baskets'\n",
    "assert df_grouped['target'].apply(len).sum() == 23212, 'wrong target len'\n",
    "assert len(df_grouped.iloc[0]['target']) == 5, 'wrong target for first user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим супер-простую модель которая состоит из\n",
    "\n",
    "1) Эмбединга айтемов\n",
    "\n",
    "2) Эмбединг корзины как среднее эмбединга всех айтемов\n",
    "\n",
    "3) RNN через все корзины\n",
    "\n",
    "4) Скор для юзера-айтема как скалярное произведение скрытого состояния RNN и эмбединга айтемов\n",
    "\n",
    "5) Нормируем софтмаксом\n",
    "\n",
    "6) BCE-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def pad_3d_sequence(tokens: List[List[int]]) -> torch.Tensor:\n",
    "    '''\n",
    "    Examples:\n",
    "    ---------\n",
    "        pad_3d_sequence(\n",
    "            [[[1, 2, 3], [4, 5]], [[3, 4], [7, 8, 9, 6], [1, 2, 3]]]\n",
    "        )\n",
    "        tensor([[[1., 2., 3., 0.],\n",
    "                 [4., 5., 0., 0.],\n",
    "                 [0., 0., 0., 0.]],\n",
    "                [[3., 4., 0., 0.],\n",
    "                 [7., 8., 9., 6.],\n",
    "                 [1., 2., 3., 0.]]])\n",
    "    '''\n",
    "    # Adopted from: https://discuss.pytorch.org/t/nested-list-of-variable-length-to-a-tensor/38699\n",
    "    words = max([len(row) for batch in tokens for row in batch])\n",
    "    sentences = max([len(batch) for batch in tokens])\n",
    "    padded = [batch + [[0] * (words)] * (sentences - len(batch)) for batch in tokens]\n",
    "    padded = torch.LongTensor([row + [0] * (words - len(row)) for batch in padded for row in batch])\n",
    "    padded = padded.view(-1, sentences, words)\n",
    "    \n",
    "    return padded\n",
    "\n",
    "print(pad_3d_sequence([\n",
    "    [[1, 2, 3], [4, 5]], \n",
    "    [[3, 4], [7, 8, 9, 6], [1, 2, 3]]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetInstances(Dataset):\n",
    "    def __init__(self, df, n_items) -> None:\n",
    "        super().__init__()\n",
    "        self._df = df\n",
    "        self._n_items = n_items\n",
    "        \n",
    "    def _to_binary(self, target):\n",
    "        binary_target = np.zeros(self._n_items)\n",
    "        binary_target[target] = 1.\n",
    "        \n",
    "        return binary_target\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        train = self._df.iloc[idx]['baskets']\n",
    "        target = self._df.iloc[idx]['target']\n",
    "        return train, len(train), self._to_binary(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._df)\n",
    "\n",
    "def collate_func_train_dataloader(batch):\n",
    "    return pad_3d_sequence([i[0] for i in batch]), torch.tensor([i[1] for i in batch]), torch.tensor([i[2] for i in batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_items, emb_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeding = torch.nn.Embedding(\n",
    "            num_embeddings=n_items, \n",
    "            embedding_dim=emb_dim,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=emb_dim, \n",
    "            hidden_size=emb_dim,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "    \n",
    "    def forward(self, \n",
    "                batch, # ~ [batch_size, n_baskets, n_items_in_basket]\n",
    "                lengths, # ~ [batch_size]\n",
    "               ):\n",
    "        # First you should get embeding for each item in each basket\n",
    "        # after that get basket embeding as average of all items in basket\n",
    "        # run RNN throw all baskets\n",
    "        # get correct last hidden state for each user(dont forget about padding)\n",
    "        # count logits as dot product between hidden state and all items embedings\n",
    "        # normalize logits with softmax and return it from model\n",
    "        # outpput ~ [batch_size, n_items]\n",
    "        # GOOD LUCK!\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, dataloader, loss_func):\n",
    "    model.train()\n",
    "    loss = 0\n",
    "    for train, lengths, target in tqdm(dataloader):\n",
    "        model.zero_grad()\n",
    "        scores = model(train, lengths)\n",
    "        batch_loss = loss_func(scores.float(), target.float())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += batch_loss.data\n",
    "    \n",
    "    return loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = df['item_id'].max() + 1\n",
    "model = Model(n_items=n_items, emb_dim=64)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=DatasetInstances(df_grouped, n_items),\n",
    "    batch_size=4,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_func_train_dataloader,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Training. Epoch {epoch}')\n",
    "    train_loss = train_one_epoch(model, optimizer, train_dataloader, loss_func)\n",
    "    print(f'Train loss: {train_loss}')\n",
    "    print(f'Epoch {epoch} is finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_loss < 0.018, \"i'm sorry for your loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(model, dataloader, top_k=20):\n",
    "    # return top_k items with highest score for each user from model\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = get_recommendations(model, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(model_predict) == len(df_grouped), 'wrong number of users'\n",
    "assert len(model_predict[0]) == 20, 'wrong number of recommendations'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрики качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HR@k - доля пользователей, у которых было хотя бы одно верное предсказание среди первых k\n",
    "\n",
    "$ HR = \\dfrac{hits}{hits + misses} $\n",
    "\n",
    "$precision = \\dfrac{TP}{TP + FP}$\n",
    "\n",
    "$recall = \\dfrac{TP}{TP + FN}$\n",
    "\n",
    "$NDCG@k = \\dfrac{DCG@k}{IDCG@k}$,где\n",
    "\n",
    "$DCG@k = \\sum_{i=1}^{k} \\dfrac{rel_i}{\\log(i + 1)}$\n",
    "\n",
    "$IDCG@k = DCG@k$ при идеальном ранжировании"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HR(predict, target, at_k=10):\n",
    "    # YOUR CODE HERE    \n",
    "    raise NotImplementedError\n",
    "\n",
    "def recall(predict, target, at_k=10):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def precision(predict, target, at_k=10):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    \n",
    "def NDCG(predict, target, at_k=10):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_pred = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
    "tmp_target = [[3, 4, 8, 9], [6, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(HR(tmp_pred, tmp_target, 1), 0.5), 'wrong HR@1'\n",
    "assert np.allclose(HR(tmp_pred, tmp_target, 2), 0.5), 'wrong HR@2'\n",
    "assert np.allclose(HR(tmp_pred, tmp_target, 3), 1.), 'wrong HR@3'\n",
    "\n",
    "assert np.allclose(recall(tmp_pred, tmp_target, 1), 0.25), 'wrong recall@1'\n",
    "assert np.allclose(recall(tmp_pred, tmp_target, 3), 0.375), 'wrong recall@3'\n",
    "assert np.allclose(recall(tmp_pred, tmp_target, 5), 0.75), 'wrong recall@5'\n",
    "\n",
    "assert np.allclose(precision(tmp_pred, tmp_target, 1), 0.5), 'wrong precision@1'\n",
    "assert np.allclose(precision(tmp_pred, tmp_target, 2), 0.25), 'wrong precision@2'\n",
    "assert np.allclose(precision(tmp_pred, tmp_target, 3), 0.3333333), 'wrong precision@3'\n",
    "\n",
    "assert np.allclose(NDCG(tmp_pred, tmp_target, 1), 0.5), 'wrong ndcg@1'\n",
    "assert np.allclose(NDCG(tmp_pred, tmp_target, 3), 0.423893), 'wrong ndcg@3'\n",
    "assert np.allclose(NDCG(tmp_pred, tmp_target, 5), 0.606831), 'wrong ndcg@5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "n_users = 10000\n",
    "n_items = 100\n",
    "\n",
    "sample_pred = [random.sample(range(1, n_items + 1), 20) for i in range(n_users)]\n",
    "sample_target = [random.sample(range(1, n_items + 1), random.randint(3, 25)) for i in range(n_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(HR(sample_pred, sample_target, 1), 0.143000), 'wrong HR@1'\n",
    "assert np.allclose(HR(sample_pred, sample_target, 3), 0.356400), 'wrong HR@3'\n",
    "assert np.allclose(HR(sample_pred, sample_target, 10), 0.731400), 'wrong HR@10'\n",
    "\n",
    "assert np.allclose(recall(sample_pred, sample_target, 1), 0.010282), 'wrong recall@1'\n",
    "assert np.allclose(recall(sample_pred, sample_target, 3), 0.030184), 'wrong recall@3'\n",
    "assert np.allclose(recall(sample_pred, sample_target, 10), 0.100828), 'wrong recall@10'\n",
    "\n",
    "assert np.allclose(precision(sample_pred, sample_target, 1), 0.143000), 'wrong precision@1'\n",
    "assert np.allclose(precision(sample_pred, sample_target, 3), 0.140633), 'wrong precision@3'\n",
    "assert np.allclose(precision(sample_pred, sample_target, 10), 0.141110), 'wrong precision@10'\n",
    "\n",
    "assert np.allclose(NDCG(sample_pred, sample_target, 1), 0.143000), 'wrong ndcg@1'\n",
    "assert np.allclose(NDCG(sample_pred, sample_target, 3), 0.141008), 'wrong ndcg@3'\n",
    "assert np.allclose(NDCG(sample_pred, sample_target, 10), 0.147899), 'wrong ndcg@10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Опять бейзлайны ಠ‿ಠ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_users = set(df_grouped['user_id'])\n",
    "df = df[df['user_id'].isin(good_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_top_popular(df, top_k=20) -> np.array:\n",
    "    # output shape ~ [n_users, top_k]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "\n",
    "def user_top_popular(df, top_k=20) -> np.array:\n",
    "    # output shape ~ [n_users, top_k]\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_popular = global_top_popular(df)\n",
    "user_popular = user_top_popular(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert global_popular.shape[0] == len(df_grouped), 'wrong global popular shape'\n",
    "assert user_popular.shape[0] == len(df_grouped), 'wrong user popular shape'\n",
    "assert global_popular.shape[1] == 20, 'wrong global popular shape'\n",
    "assert global_popular[0].tolist() == [1504, 1277, 1242, 1574, 1470, 1068,  168,  654,  895,  459,  572, 466, 1200, 1412,  213, 1573,  901,  285,   27,  996]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_k = [1, 3, 5, 10]\n",
    "target = df_grouped['target'].values\n",
    "\n",
    "final_metrics = pd.DataFrame(columns=['metric', 'model', 'top_popular', 'user_popular'])\n",
    "for func in [HR, precision, recall, NDCG]:\n",
    "    for k in list_k:\n",
    "        metrics = {\n",
    "            'metric': f'{func.__name__}@{k}', \n",
    "            'model': func(model_predict, target, k), \n",
    "            'top_popular': func(global_popular, target, k), \n",
    "            'user_popular': func(user_popular, target, k),\n",
    "        }\n",
    "        final_metrics = final_metrics.append(metrics, ignore_index=True)\n",
    "        \n",
    "final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step?\n",
    "\n",
    "* Взять датасет [побольше](https://www.dunnhumby.com/source-files/#)\n",
    "\n",
    "* Реализовать другую архитектуру([DREAM](https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/A%20Dynamic%20Recurrent%20Model%20for%20Next%20Basket%20Recommendation.pdf), [BERT4Rec](https://arxiv.org/pdf/1904.06690.pdf), [RepeatNet](https://arxiv.org/pdf/1812.02646.pdf), etc)\n",
    "\n",
    "* Увеличить размерность векторов, добавить lr_scheduler, потюнить другие параметры\n",
    "\n",
    "* Улучшить Dataloader, генерировать батчи из корзин \"похожего размера\"\n",
    "\n",
    "* Попробовать более интересный лосс([BPR](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf), [WARP](http://www.thespermwhale.com/jaseweston/papers/wsabie-ijcai.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRP\n",
    "\n",
    "$$L = \\sum_{i=1}^{N} \\ln(1 + e ^ {-(s_+ - s_-)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRPLoss(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwags):\n",
    "        super().__init__()\n",
    "        #YOUR CODE HERE\n",
    "        \n",
    "    def forward(self, positives, negatives, *args, **kwags):\n",
    "        #YOUR CODE HERE\n",
    "        raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
